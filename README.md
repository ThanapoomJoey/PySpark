# ðŸ“˜ Day 1: à¸£à¸¹à¹‰à¸ˆà¸±à¸ PySpark à¹à¸¥à¸° SparkSession

## ðŸŽ¯ à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢
à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸žà¸·à¹‰à¸™à¸à¸²à¸™ Spark, à¸ªà¸£à¹‰à¸²à¸‡ SparkSession à¹à¸¥à¸°à¹‚à¸«à¸¥à¸” DataFrame

---

## ðŸ“˜ PySpark à¸„à¸·à¸­à¸­à¸°à¹„à¸£ à¸—à¸³à¹„à¸¡à¸–à¸¶à¸‡à¹ƒà¸Šà¹‰  
**PySpark** à¸„à¸·à¸­ interface à¸‚à¸­à¸‡ Apache Spark à¸ªà¸³à¸«à¸£à¸±à¸šà¸ à¸²à¸©à¸² Python  
à¹ƒà¸Šà¹‰à¸ˆà¸±à¸”à¸à¸²à¸£à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆà¹à¸šà¸š distributed à¸£à¸­à¸‡à¸£à¸±à¸šà¸‡à¸²à¸™ ETL, Machine Learning, Analytics  
à¸ˆà¸¸à¸”à¹€à¸”à¹ˆà¸™à¸„à¸·à¸­à¸—à¸³à¸‡à¸²à¸™à¹€à¸£à¹‡à¸§ à¸à¸£à¸°à¸ˆà¸²à¸¢à¹‚à¸«à¸¥à¸”à¹„à¸”à¹‰à¸«à¸¥à¸²à¸¢à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¹à¸šà¸š scale-out (à¸«à¸¥à¸²à¸¢ node)

---

## âš™ï¸ SparkSession à¸„à¸·à¸­à¸­à¸°à¹„à¸£  
`SparkSession` à¸„à¸·à¸­à¸ˆà¸¸à¸”à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸‚à¸­à¸‡à¸à¸²à¸£à¹ƒà¸Šà¹‰ PySpark  
à¹ƒà¸Šà¹‰à¹€à¸žà¸·à¹ˆà¸­à¸ªà¸·à¹ˆà¸­à¸ªà¸²à¸£à¸à¸±à¸š Spark engine à¹à¸¥à¸°à¹ƒà¸Šà¹‰à¹‚à¸«à¸¥à¸”/à¸ˆà¸±à¸”à¸à¸²à¸£à¸‚à¹‰à¸­à¸¡à¸¹à¸¥

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("MyFirstApp").getOrCreate()
 ```

## ðŸ“‚ à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œ CSV / à¸ªà¸£à¹‰à¸²à¸‡ DataFrame à¸ˆà¸²à¸ Python dict
**à¸ˆà¸²à¸à¹„à¸Ÿà¸¥à¹Œ CSV:**
```python
df = spark.read.csv("/FileStore/tables/supermarket_sales_20241231.csv", header=True , inferSchema=True)
 ```

###### `header=True`
à¸žà¸²à¸£à¸²à¸¡à¸´à¹€à¸•à¸­à¸£à¹Œà¸™à¸µà¹‰à¸šà¸­à¸à¹ƒà¸«à¹‰ PySpark à¹ƒà¸Šà¹‰à¹à¸–à¸§à¹à¸£à¸à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ CSV à¹€à¸›à¹‡à¸™à¸Šà¸·à¹ˆà¸­à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ  
à¸–à¹‰à¸²à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¹€à¸›à¹‡à¸™ `False` PySpark à¸ˆà¸°à¸–à¸·à¸­à¸§à¹ˆà¸²à¹à¸–à¸§à¹à¸£à¸à¹€à¸›à¹‡à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸›à¸à¸•à¸´ à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆà¸Šà¸·à¹ˆà¸­à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ

###### `inferSchema=True`
à¹ƒà¸Šà¹‰à¹ƒà¸«à¹‰ PySpark à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸›à¸£à¸°à¹€à¸ à¸—à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹‚à¸”à¸¢à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´  
à¹€à¸Šà¹ˆà¸™ à¸–à¹‰à¸²à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸™à¸±à¹‰à¸™à¹† à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸¥à¸‚ PySpark à¸ˆà¸°à¸à¸³à¸«à¸™à¸”à¸›à¸£à¸°à¹€à¸ à¸—à¹€à¸›à¹‡à¸™ `IntegerType` à¸«à¸£à¸·à¸­ `DoubleType` à¹€à¸›à¹‡à¸™à¸•à¹‰à¸™  
à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¹€à¸›à¹‡à¸™ `True` à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆà¸£à¸°à¸šà¸¸ à¸„à¹ˆà¸²à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸ˆà¸°à¹€à¸›à¹‡à¸™à¸à¸²à¸£à¸­à¹ˆà¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹€à¸›à¹‡à¸™ `String` (à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡)

**à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆ Upload à¹ƒà¸™ dbft**
```python
%fs ls dbfs:/FileStore/tables/
 ```

**à¸¥à¸šà¹„à¸Ÿà¸¥à¹Œ à¹ƒà¸™ dbfs**
```python
%fs rm -r  dbfs:/FileStore/tables/supermarket_sales_20241231.csv
 ```
```python
%fs rm -r dbfs:/FileStore/tables/supermarket_sales_20241231.parquet/
 ```

**à¸ˆà¸²à¸ list à¸‚à¸­à¸‡ dict:**
```python
data = [{"name": "John", "age": 30}, {"name": "Jane", "age": 25}]
df = spark.createDataFrame(data)
 ```

**à¸ˆà¸²à¸ List of Tuples + Schema**
```python
data = [("John", 30), ("Jane", 25)]
schema = ["name", "age"]
df = spark.createDataFrame(data, schema)
 ```

## ðŸ” à¹ƒà¸Šà¹‰ .show() à¹à¸¥à¸° .printSchema()
```python
df.show(5)           # à¹à¸ªà¸”à¸‡ 5 à¹à¸–à¸§à¹à¸£à¸à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
df.printSchema()     # à¹à¸ªà¸”à¸‡ schema à¸‚à¸­à¸‡ DataFrame
df.dtypes            # Output: [('Name', 'string'), ('Age', 'bigint')]
print((df.count(), len(df.columns)))  # (10000000, 8)
  ```

# ðŸ“˜ Day 2: à¸„à¸³à¸ªà¸±à¹ˆà¸‡à¸žà¸·à¹‰à¸™à¸à¸²à¸™à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¸šà¹ˆà¸­à¸¢à¹ƒà¸™ PySpark

## ðŸŽ¯ à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢
à¹ƒà¸Šà¹‰à¸„à¸³à¸ªà¸±à¹ˆà¸‡à¸žà¸·à¹‰à¸™à¸à¸²à¸™à¸•à¹ˆà¸²à¸‡ à¹† à¹€à¸Šà¹ˆà¸™ `select`, `filter`, `withColumn`, `drop` à¹à¸¥à¸°à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸ˆà¸²à¸ `pyspark.sql.functions`

---

## ðŸ“‚ à¸„à¸³à¸ªà¸±à¹ˆà¸‡ `select()`, `filter()`, `where()`

- **`select()`**: à¹ƒà¸Šà¹‰à¹€à¸žà¸·à¹ˆà¸­à¹€à¸¥à¸·à¸­à¸à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸ˆà¸²à¸ DataFrame à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹à¸ªà¸”à¸‡à¸œà¸¥
    ```python
    df.select("sale_id","customer_id").show(5)
     ```
- **`filter()`** à¹à¸¥à¸° **`where()`**: à¹ƒà¸Šà¹‰à¸à¸£à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸²à¸¡à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚à¸—à¸µà¹ˆà¸à¸³à¸«à¸™à¸”
    ```python
    df2 = df.filter(df.sale_date == "2024-11-08")
    df2.show(5)
    df2.count()
     ```
    ```python
    df.where(df.sale_date == "2024-11-08")
     ```

---

## âš™ï¸ à¸à¸²à¸£à¹ƒà¸Šà¹‰ `withColumn()`, `drop()`

- **`withColumn()`**: à¹ƒà¸Šà¹‰à¹€à¸žà¸´à¹ˆà¸¡à¸«à¸£à¸·à¸­à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹ƒà¸™ DataFrame à¹‚à¸”à¸¢à¸ªà¸²à¸¡à¸²à¸£à¸–à¹ƒà¸Šà¹‰à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸«à¸£à¸·à¸­à¸„à¸³à¸™à¸§à¸“à¸„à¹ˆà¸²à¸•à¹ˆà¸²à¸‡ à¹† à¹„à¸”à¹‰
    ```python
    from pyspark.sql.functions import col

    df = df.withColumn("age_plus_10", col("age") + 10)
    df.show()
     ```
    à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ `withColumn()` à¸­à¸·à¹ˆà¸™à¹†à¸”à¸¹à¹„à¸”à¹‰à¸—à¸µà¹ˆ : [withColumn.md](withColumn.md)

- **`drop()`**: à¹ƒà¸Šà¹‰à¹€à¸žà¸·à¹ˆà¸­à¸¥à¸šà¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸­à¸­à¸à¸ˆà¸²à¸ DataFrame
    ```python
    df = df.drop("age_plus_10")
    df.show()
     ```

---

## ðŸ”¢ à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¹ƒà¸™ `pyspark.sql.functions`

- **`col()`**: à¹ƒà¸Šà¹‰à¹€à¸¥à¸·à¸­à¸à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸ˆà¸²à¸ DataFrame à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰à¸Šà¸·à¹ˆà¸­à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ
    ```python
    from pyspark.sql.functions import col
    df.select(col("name"), col("age")).show()
     ```

- **`lit()`**: à¹ƒà¸Šà¹‰à¹€à¸žà¸´à¹ˆà¸¡à¸„à¹ˆà¸²à¸„à¸‡à¸—à¸µà¹ˆà¹ƒà¸™ DataFrame à¹‚à¸”à¸¢à¸ªà¸²à¸¡à¸²à¸£à¸–à¸™à¸³à¸„à¹ˆà¸²à¸„à¸‡à¸—à¸µà¹ˆà¸¡à¸²à¹ƒà¸Šà¹‰à¸£à¹ˆà¸§à¸¡à¸à¸±à¸šà¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ
    ```python
    from pyspark.sql.functions import lit
    df = df.withColumn("constant_value", lit(100))
    df.show()
     ```
- **`when()`**: à¹ƒà¸Šà¹‰à¹€à¸žà¸·à¹ˆà¸­à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹ƒà¸«à¸¡à¹ˆà¹‚à¸”à¸¢à¸à¸²à¸£à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡à¸„à¹ˆà¸²à¸•à¸²à¸¡à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚ à¹€à¸Šà¹ˆà¸™ à¸à¸²à¸£à¹ƒà¸Šà¹‰ `if-else` à¹ƒà¸™ DataFrame
    ```python
    from pyspark.sql.functions import when
    df = df.withColumn("age_group", when(df.age > 30, "à¸ªà¸¹à¸‡à¸à¸§à¹ˆà¸² 30").otherwise("à¸•à¹ˆà¸³à¸à¸§à¹ˆà¸² 30"))
    df.show()
     ```

---

# ðŸ“˜ Day 3: GroupBy, Aggregate, Sort

## ðŸŽ¯ à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢
- à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸à¸²à¸£à¹ƒà¸Šà¹‰ `.groupBy()` à¹à¸¥à¸°à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™ Aggregate à¹€à¸Šà¹ˆà¸™ `count()`, `sum()`, `avg()`
- à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸à¸²à¸£à¸ˆà¸±à¸”à¹€à¸£à¸µà¸¢à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸”à¹‰à¸§à¸¢ `.orderBy()`
- à¸—à¸³ Mini Project à¸—à¸µà¹ˆà¸£à¸§à¸¡à¸—à¸¸à¸à¸­à¸¢à¹ˆà¸²à¸‡

---

## ðŸ§  à¹€à¸™à¸·à¹‰à¸­à¸«à¸²

### 1. `groupBy() + agg()`

à¹ƒà¸Šà¹‰à¸ªà¸³à¸«à¸£à¸±à¸š **à¸ˆà¸±à¸”à¸à¸¥à¸¸à¹ˆà¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥** à¸•à¸²à¸¡à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ à¹à¸¥à¹‰à¸§à¹ƒà¸Šà¹‰à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™ aggregate à¹€à¸žà¸·à¹ˆà¸­à¸ªà¸£à¸¸à¸›à¸‚à¹‰à¸­à¸¡à¸¹à¸¥

```python
from pyspark.sql.functions import count, sum, avg, max, min

df.groupBy("category").agg(
    count("*").alias("total_rows"),
    sum("sales").alias("total_sales"),
    avg("sales").alias("avg_sales")
).show()
```

---

### 2. à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™ Aggregate à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸

| à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™ | à¹ƒà¸Šà¹‰à¸—à¸³à¸­à¸°à¹„à¸£ |
|----------|-----------|
| `count()` | à¸™à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™à¹à¸–à¸§ |
| `sum()` | à¸£à¸§à¸¡à¸„à¹ˆà¸²à¸•à¸±à¸§à¹€à¸¥à¸‚ |
| `avg()` | à¸«à¸²à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢ |
| `max()` | à¸«à¸²à¸„à¹ˆà¸²à¸ªà¸¹à¸‡à¸ªà¸¸à¸” |
| `min()` | à¸«à¸²à¸„à¹ˆà¸²à¸•à¹ˆà¸³à¸ªà¸¸à¸” |

---

### 3. `orderBy()` à¸à¸²à¸£à¸ˆà¸±à¸”à¹€à¸£à¸µà¸¢à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥

```python
df.orderBy("sales").show()  # à¹€à¸£à¸µà¸¢à¸‡à¸ˆà¸²à¸à¸™à¹‰à¸­à¸¢à¹„à¸›à¸¡à¸²à¸
df.orderBy(df.sales.desc()).show()  # à¹€à¸£à¸µà¸¢à¸‡à¸ˆà¸²à¸à¸¡à¸²à¸à¹„à¸›à¸™à¹‰à¸­à¸¢
```

**à¸«à¸¥à¸²à¸¢à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚:**
```python
df.orderBy(df.category.asc(), df.sales.desc()).show()
```

---

### 4. à¸à¸£à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¥à¸±à¸‡ groupBy à¸”à¹‰à¸§à¸¢ `filter()`

```python
agg_df = df.groupBy("category").agg(sum("sales").alias("total_sales"))
agg_df.filter(agg_df.total_sales > 1000).show()
```

---

### 5. à¹ƒà¸Šà¹‰ `.alias()` à¸•à¸±à¹‰à¸‡à¸Šà¸·à¹ˆà¸­à¹ƒà¸«à¸¡à¹ˆà¹ƒà¸«à¹‰à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ

```python
df.groupBy("region").agg(avg("score").alias("average_score")).show()
```
