# üìò Day 1: ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å PySpark ‡πÅ‡∏•‡∏∞ SparkSession

## üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô Spark, ‡∏™‡∏£‡πâ‡∏≤‡∏á SparkSession ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î DataFrame

---

## üìò PySpark ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£ ‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡πÉ‡∏ä‡πâ  
**PySpark** ‡∏Ñ‡∏∑‡∏≠ interface ‡∏Ç‡∏≠‡∏á Apache Spark ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤ Python  
‡πÉ‡∏ä‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏ö‡∏ö distributed ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô ETL, Machine Learning, Analytics  
‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏£‡πá‡∏ß ‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏ö‡∏ö scale-out (‡∏´‡∏•‡∏≤‡∏¢ node)

---

## ‚öôÔ∏è SparkSession ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£  
`SparkSession` ‡∏Ñ‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ PySpark  
‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Å‡∏±‡∏ö Spark engine ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡πÇ‡∏´‡∏•‡∏î/‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("MyFirstApp").getOrCreate()
 ```

## üìÇ ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV / ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏à‡∏≤‡∏Å Python dict
**‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV:**
```python
df = spark.read.csv("/FileStore/tables/supermarket_sales_20241231.csv", header=True , inferSchema=True)
 ```

###### `header=True`
‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ‡∏ö‡∏≠‡∏Å‡πÉ‡∏´‡πâ PySpark ‡πÉ‡∏ä‡πâ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå  
‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô `False` PySpark ‡∏à‡∏∞‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå

###### `inferSchema=True`
‡πÉ‡∏ä‡πâ‡πÉ‡∏´‡πâ PySpark ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥  
‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç PySpark ‡∏à‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏õ‡πá‡∏ô `IntegerType` ‡∏´‡∏£‡∏∑‡∏≠ `DoubleType` ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô  
‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô `True` ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô `String` (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà Upload ‡πÉ‡∏ô dbft**
```python
%fs ls dbfs:/FileStore/tables/
 ```

**‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå ‡πÉ‡∏ô dbfs**
```python
%fs rm -r  dbfs:/FileStore/tables/supermarket_sales_20241231.csv
 ```
```python
%fs rm -r dbfs:/FileStore/tables/supermarket_sales_20241231.parquet/
 ```

**‡∏à‡∏≤‡∏Å list ‡∏Ç‡∏≠‡∏á dict:**
```python
data = [{"name": "John", "age": 30}, {"name": "Jane", "age": 25}]
df = spark.createDataFrame(data)
 ```

**‡∏à‡∏≤‡∏Å List of Tuples + Schema**
```python
data = [("John", 30), ("Jane", 25)]
schema = ["name", "age"]
df = spark.createDataFrame(data, schema)
 ```

## üîç ‡πÉ‡∏ä‡πâ .show() ‡πÅ‡∏•‡∏∞ .printSchema()
```python
df.show(5)           # ‡πÅ‡∏™‡∏î‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
df.printSchema()     # ‡πÅ‡∏™‡∏î‡∏á schema ‡∏Ç‡∏≠‡∏á DataFrame
df.dtypes            # Output: [('Name', 'string'), ('Age', 'bigint')]
print((df.count(), len(df.columns)))  # (10000000, 8)
  ```

# üìò Day 2: ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô PySpark

## üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÄ‡∏ä‡πà‡∏ô `select`, `filter`, `withColumn`, `drop` ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≤‡∏Å `pyspark.sql.functions`

---

## üìÇ ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á `select()`, `filter()`, `where()`

- **`select()`**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏≤‡∏Å DataFrame ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
    ```python
    df.select("sale_id","customer_id").show(5)
     ```
- **`filter()`** ‡πÅ‡∏•‡∏∞ **`where()`**: ‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    ```python
    df2 = df.filter(df.sale_date == "2024-11-08")
    df2.show(5)
    df2.count()
     ```
    ```python
    df.where(df.sale_date == "2024-11-08")
     ```

---

## ‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ `withColumn()`, `drop()`

- **`withColumn()`**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô DataFrame ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÑ‡∏î‡πâ
    ```python
    from pyspark.sql.functions import col

    df = df.withColumn("age_plus_10", col("age") + 10)
    df.show()
     ```
    ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô `withColumn()` ‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡∏î‡∏π‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà : [withColumn.md](withColumn.md)

- **`drop()`**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å DataFrame
    ```python
    df = df.drop("age_plus_10")
    df.show()
     ```

---

## üî¢ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏ô `pyspark.sql.functions`

- **`col()`**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏≤‡∏Å DataFrame ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    ```python
    from pyspark.sql.functions import col
    df.select(col("name"), col("age")).show()
     ```

- **`lit()`**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ô DataFrame ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    ```python
    from pyspark.sql.functions import lit
    df = df.withColumn("constant_value", lit(100))
    df.show()
     ```
- **`when()`**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ `if-else` ‡πÉ‡∏ô DataFrame
    ```python
    from pyspark.sql.functions import when
    df = df.withColumn("age_group", when(df.age > 30, "‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ 30").otherwise("‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 30"))
    df.show()
     ```

---

# üìò Day 3: GroupBy, Aggregate, Sort

## üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ `.groupBy()` ‡πÅ‡∏•‡∏∞‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Aggregate ‡πÄ‡∏ä‡πà‡∏ô `count()`, `sum()`, `avg()`
- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ `.orderBy()`
- ‡∏ó‡∏≥ Mini Project ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á

---

## üß† ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤

### 1. `groupBy() + agg()`

‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö **‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•** ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô aggregate ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

```python
from pyspark.sql.functions import count, sum, avg, max, min

df.groupBy("category").agg(
    count("*").alias("total_rows"),
    sum("sales").alias("total_sales"),
    avg("sales").alias("avg_sales")
).show()
```

---

### 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Aggregate ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

| ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô | ‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£ |
|----------|-----------|
| `count()` | ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß |
| `sum()` | ‡∏£‡∏ß‡∏°‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç |
| `avg()` | ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ |
| `max()` | ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î |
| `min()` | ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î |

```python
from pyspark.sql.functions import sum , format_number

df.groupBy("branch_id").agg(format_number(sum("total_amount"),2).alias("total_branch_sales")).show()
 ```
---

### 3. `orderBy()` ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

```python
df.orderBy("sales").show()  # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÑ‡∏õ‡∏°‡∏≤‡∏Å
df.orderBy(col("sales").desc()).show()  # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢ ‡πÅ‡∏ö‡∏ö 1
df.orderBy(col("branch_id"),ascending = False).show() # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢ ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà 2
```

**‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç:**
```python
df.orderBy(df.category.asc(), df.sales.desc()).show()
```

---

### 4. ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á groupBy ‡∏î‡πâ‡∏ß‡∏¢ `filter()`

```python
agg_df = df.groupBy("category").agg(sum("sales").alias("total_sales"))
agg_df.filter(agg_df.total_sales > 1000).show()
```

---

### 5. ‡πÉ‡∏ä‡πâ `.alias()` ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå

```python
df.groupBy("region").agg(avg("score").alias("average_score")).show()
```

**‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô**
```python
from pyspark.sql.functions import sum , col

df.groupBy(col("sale_date") , col("branch_id"))\
  .agg(sum("quantity").alias("total_quantity"))\
  .orderBy(col("sale_date").asc(),col("total_quantity").desc())\
  .filter(col("total_quantity") > 25000).show()
 ```

# üìò Day 4: Join + SQL  

## üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:
- ‡πÉ‡∏ä‡πâ `.join()` ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 2 ‡∏ä‡∏∏‡∏î
- ‡πÉ‡∏ä‡πâ SQL ‡∏ú‡πà‡∏≤‡∏ô `spark.sql()` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô query ‡∏ö‡∏ô DataFrame
- ‡∏ó‡∏≥ Mini Project ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ù‡∏∂‡∏Å Join + SQL ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô

---

## üß† ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤

### üîπ 1. PySpark `.join()`

```python
df1.join(df2, on="id", how="inner")
 ```

### üîπ 2. ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á Join

| ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó Join | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ |
|-------------|----------|
| `inner`     | ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà match ‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á 2 ‡∏ù‡∏±‡πà‡∏á |
| `left`      | ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß‡∏à‡∏≤‡∏Å‡∏ù‡∏±‡πà‡∏á‡∏ã‡πâ‡∏≤‡∏¢ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà match ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô NULL |
| `right`     | ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß‡∏à‡∏≤‡∏Å‡∏ù‡∏±‡πà‡∏á‡∏Ç‡∏ß‡∏≤ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà match ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô NULL |
| `outer`     | ‡πÅ‡∏™‡∏î‡∏á ‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà ‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á ‡πÅ‡∏°‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ match ‡∏Å‡πá‡∏ï‡∏≤‡∏° |

```python
df1.join(df2, df1.id == df2.id, how="left")
 ```

---

### üîπ 3. ‡πÅ‡∏Å‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥‡∏Å‡πà‡∏≠‡∏ô Join

```python
df1 = df1.withColumnRenamed("name", "name_df1")
df2 = df2.withColumnRenamed("name", "name_df2")
 ```

---

### üîπ 4. ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ SQL ‡∏ö‡∏ô DataFrame

```python
sales_df.createOrReplaceTempView("temp_sale")
product_df.createOrReplaceTempView("temp_product")

spark.sql("""
          select s.customer , s.sale_date, sum((p.price - p.cost)*s.quantity) as total_profit , sum(s.quantity * p.price) as total_sale
          from temp_sale s
          left join temp_product p
            on s.product_id = p.product_id
          group by s.customer,s.sale_date
          """).show()
 ```

---
