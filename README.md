# üìò Day 1: ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å PySpark ‡πÅ‡∏•‡∏∞ SparkSession

## üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô Spark, ‡∏™‡∏£‡πâ‡∏≤‡∏á SparkSession ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î DataFrame

---

## üìò PySpark ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£ ‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡πÉ‡∏ä‡πâ  
**PySpark** ‡∏Ñ‡∏∑‡∏≠ interface ‡∏Ç‡∏≠‡∏á Apache Spark ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤ Python  
‡πÉ‡∏ä‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏ö‡∏ö distributed ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô ETL, Machine Learning, Analytics  
‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏£‡πá‡∏ß ‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏ö‡∏ö scale-out (‡∏´‡∏•‡∏≤‡∏¢ node)

---

## ‚öôÔ∏è SparkSession ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£  
`SparkSession` ‡∏Ñ‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ PySpark  
‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Å‡∏±‡∏ö Spark engine ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡πÇ‡∏´‡∏•‡∏î/‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("MyFirstApp").getOrCreate()
 ```

## üìÇ ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV / ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏à‡∏≤‡∏Å Python dict
**‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV:**
```python
df = spark.read.csv("/FileStore/tables/supermarket_sales_20241231.csv", header=True , inferSchema=True)
 ```

###### `header=True`
‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ‡∏ö‡∏≠‡∏Å‡πÉ‡∏´‡πâ PySpark ‡πÉ‡∏ä‡πâ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå  
‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô `False` PySpark ‡∏à‡∏∞‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå

###### `inferSchema=True`
‡πÉ‡∏ä‡πâ‡πÉ‡∏´‡πâ PySpark ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥  
‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç PySpark ‡∏à‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏õ‡πá‡∏ô `IntegerType` ‡∏´‡∏£‡∏∑‡∏≠ `DoubleType` ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô  
‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô `True` ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô `String` (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà Upload ‡πÉ‡∏ô dbft**
```python
%fs ls dbfs:/FileStore/tables/
 ```

**‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå ‡πÉ‡∏ô dbfs**
```python
%fs rm -r  dbfs:/FileStore/tables/supermarket_sales_20241231.csv
 ```
```python
%fs rm -r dbfs:/FileStore/tables/supermarket_sales_20241231.parquet/
 ```

**‡∏à‡∏≤‡∏Å list ‡∏Ç‡∏≠‡∏á dict:**
```python
data = [{"name": "John", "age": 30}, {"name": "Jane", "age": 25}]
df = spark.createDataFrame(data)
 ```

**‡∏à‡∏≤‡∏Å List of Tuples + Schema**
```python
data = [("John", 30), ("Jane", 25)]
schema = ["name", "age"]
df = spark.createDataFrame(data, schema)
 ```

## üîç ‡πÉ‡∏ä‡πâ .show() ‡πÅ‡∏•‡∏∞ .printSchema()
```python
df.show(5)           # ‡πÅ‡∏™‡∏î‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
df.printSchema()     # ‡πÅ‡∏™‡∏î‡∏á schema ‡∏Ç‡∏≠‡∏á DataFrame
df.dtypes            # Output: [('Name', 'string'), ('Age', 'bigint')]
print((df.count(), len(df.columns)))  # (10000000, 8)
  ```

# üìò Day 2: ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô PySpark

## üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÄ‡∏ä‡πà‡∏ô `select`, `filter`, `withColumn`, `drop` ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≤‡∏Å `pyspark.sql.functions`

---

## üìÇ ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á `select()`, `filter()`, `where()`

- **`select()`**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏≤‡∏Å DataFrame ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
    ```python
    df.select("sale_id","customer_id").show(5)
     ```
- **`filter()`** ‡πÅ‡∏•‡∏∞ **`where()`**: ‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    ```python
    df2 = df.filter(df.sale_date == "2024-11-08")
    df2.show(5)
    df2.count()
     ```
    ```python
    df.where(df.sale_date == "2024-11-08")
     ```

---

## ‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ `withColumn()`, `drop()`

- **`withColumn()`**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô DataFrame ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÑ‡∏î‡πâ
    ```python
    from pyspark.sql.functions import col

    df = df.withColumn("age_plus_10", col("age") + 10)
    df.show()
     ```
    ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô `withColumn()` ‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡∏î‡∏π‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà : [withColumn.md](withColumn.md)

- **`drop()`**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å DataFrame
    ```python
    df = df.drop("age_plus_10")
    df.show()
     ```

---

## üî¢ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏ô `pyspark.sql.functions`

- **`col()`**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏≤‡∏Å DataFrame ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    ```python
    from pyspark.sql.functions import col
    df.select(col("name"), col("age")).show()
     ```

- **`lit()`**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ô DataFrame ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    ```python
    from pyspark.sql.functions import lit
    df = df.withColumn("constant_value", lit(100))
    df.show()
     ```
- **`when()`**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ `if-else` ‡πÉ‡∏ô DataFrame
    ```python
    from pyspark.sql.functions import when
    df = df.withColumn("age_group", when(df.age > 30, "‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ 30").otherwise("‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 30"))
    df.show()
     ```

---
